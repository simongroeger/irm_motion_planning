<!DOCTYPE html>
<html lang="en"> 
<head>
    <title>Motion Planning in RKHS</title>
    
    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Blog Template">
    <meta name="author" content="Xiaoying Riley at 3rd Wave Media">    
    <link rel="shortcut icon" href="favicon.ico"> 
    
    <!-- FontAwesome JS-->
	<script defer src="assets/fontawesome/js/all.min.js"></script>
    
    <!-- Plugin CSS -->
    <link rel="stylesheet" href="assets/plugins/highlight/styles/monokai-sublime.css">
    
    <!-- Theme CSS -->  
    <link id="theme-style" rel="stylesheet" href="assets/css/theme-1.css">
    
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head> 

<body>
    
    <header class="header text-center">	    
	    <h1 class="blog-name pt-lg-4 mb-0"><a class="no-text-decoration" >Simon's Blog</a></h1>
        
	    <nav class="navbar navbar-expand-lg navbar-dark" >
           
			<button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navigation" aria-controls="navigation" aria-expanded="false" aria-label="Toggle navigation">
				<span class="navbar-toggler-icon"></span>
			</button>

			<div id="navigation" class="collapse navbar-collapse flex-column" >
				<div class="profile-section pt-3 pt-lg-4">
				    <img class="profile-image mb-3 rounded-circle mx-auto" src="assets/images/profile.jpg" alt="image" >			
					
					<div class="bio mb-3">Hi, my name is Simon Gröger. I am a student in my computer science master at TU Darmstadt and currently working on a  trajectory optimization project.<br></div><!--//bio-->
					<hr> 
					<div>
						<h3 class="navbio pt-lg-4 mb-0"><a class="no-text-decoration" href="#" >Motion Planning in RKHS</a></h3>
						<h5 class="navbio pt-lg-2 mb-0"><a class="no-text-decoration" href="#Introduction" >Introduction</a></h5>
						<h5 class="navbio pt-lg-1 mb-0"><a class="no-text-decoration" href="#Math" >Math</a></h5>
						<h5 class="navbio pt-lg-1 mb-0"><a class="no-text-decoration" href="#Implementation" >Implementation</a></h5>
						<h5 class="navbio pt-lg-1 mb-0"><a class="no-text-decoration" href="#Experiments" >Experiments</a></h5>
						<h5 class="navbio pt-lg-1 mb-0"><a class="no-text-decoration" href="#Insights" >Insights</a></h5>
						<h5 class="navbio pt-lg-1 mb-0"><a class="no-text-decoration" href="#Conclusion" >Conclusion</a></h5>
					</div>
				</div><!--//profile-section-->
			</div>
		</nav>
    </header>
    
    <div class="main-wrapper">
	    
<article class="blog-post px-3 py-5 p-md-5">
	<div class="container single-col-max-width">


<!-- 
Actual content 
..........................................................................
..........................................................................
..........................................................................
..........................................................................
..........................................................................
..........................................................................
..........................................................................
-->

<header class="blog-post-header">
	<h1 class="title mb-2"><span id="Motion Planning in RKHS"></span>Motion Planning in RKHS</h1>
	<div class="meta mb-3"><span class="date">Intelligent Robotic Manipulation Project Lab II</span><span class="date">April 24 - Oktober 24</span><span class="date">Simon Gröger</span></div>
</header>
			    
<div class="blog-post-body">
<figure class="blog-banner">
	<img class="img-fluid" src="assets/images/heading.png" alt="image">
	<figcaption class="mt-2 text-center image-caption">Trajectory Optimization with a 3-link-robot</figcaption>
</figure>

<p>
	In this project, we develop a trajectory optimization approach using the Reproducing Kernel Hilbert Space. 
	We use JAX and its just-in-time (JIT) compilation and analytical gradients to reduce the runtime to 3.1ms and enable robot deployment.
	We use a constraint optimization problem for the trajectory to reach both the start and goal configurations while respecting the robot's physical limitations.
</p>

<h2 class="mt-5 mb-3"><span id="Introduction"></span>Introduction</h2>

<p>
	In motion planning, the trajectories are often defined in the Reproducing Kernel Hilbert Space (RKHS) and initialized in a straight line from the start to the goal configuration. 
	By minimizing the cost of the trajectory using functional-gradient descent we avoid obstacles.
	Using the RKHS and functional gradients usually results better as the problem formulation has fewer local optima.
</p>

<h2 class="mt-5 mb-3"><span id="Math"></span>Math</h2>

<h4 class="mt-3 mb-3">Trajectory Formulation</h4>

<p> 

	Our trajectory, defined in RKHS, is parameterized between 0 and 1 and includes a parameter matrix \(\alpha\), a kernel matrix \(K\), and an additional matrix \(J\). 
	The kernel matrix uses the RBF kernel and the matrix J enables cross-joint communication and optimization.
	The trajectory can be evaluated to the joint position for any timestep \(t\) in \([0, 1]\) using 
	\[ 
	evaluate(K, J, \alpha, t) = K(\cdot, t) \alpha J 
	\]
	and the joint position of joint \(j\) can be received with  \( evaluate(K, J, \alpha, t)[j] \).
<br>
	For the evaluation of the joint velocity, we define an additional kernel matrix \(dK\) with \( dK = {\partial \over \partial t} K \) , such that it holds:  
	\[ 
	{\partial \over \partial t} evaluate(K, J, \alpha, t) = evaluate(dK, J, \alpha, t) 
	\]
	Thus the evaluation of the joint velocity is faster without having to differentiate each time.
</p>
<p>
	During the trajectory optimization a finite fixed set of supporting points \( t_i \) for \( i \in \{1,...,n_{support}\} \) is used to allow the use of a precomputed fixed kernel matrix to speedup the trajectory optimization.
</p>	


<h4 class="mt-5 mb-3">Constraint Loss Formulation</h4>

<p>
The trajectory optimization problem involves minimizing the trajectory's cost in the cost map and satisfying multiple trajectory constraints.
The trajectory constraints define the distance between the start and end of the trajectory and our environment's start and goal configurations. Additionally, the joint position is limited to a maximum and minimum position, and the joint velocity is limited to a maximum absolute velocity.

Therefore, the constraint formulation of our optimization problem is as follows:

\[ \min_{\alpha} L(\alpha) = \sum_{i=1}^{n_{support}} costmap( fk(evaluate(K, J, \alpha, t_i) ) + { \lambda_{reg}  \over 2 } || evaluate(K, J, \alpha, \cdot) ||^2_2 \]
s.t. 
\[ { 1 \over 2 } || evaluate(K, J, \alpha, t_1)  - config_{start} ||^2_2 \le \epsilon_{position} \]
\[ { 1 \over 2 } || evaluate(K, J, \alpha, t_{n_{support}}) - config_{goal} ||^2_2 \le \epsilon_{position}  \]
\[ { 1 \over 2 } || evaluate(dK, J, \alpha, t_1) ||^2_2 \le \epsilon_{velocity}  \]
\[ { 1 \over 2 } || evaluate(dK, J, \alpha, t_{n_{support}}) ||^2_2 \le \epsilon_{velocity}  \]
\[ \forall i \in \{1,...,n_{support}\}, \forall j \in \{1,...,n_{joints}\} : evaluate(K, J, \alpha, t_i)[j] \le jointposition_{max}  \]
\[ \forall i \in \{1,...,n_{support}\}, \forall j \in \{1,...,n_{joints}\} : evaluate(K, J, \alpha, t_i)[j] \ge jointposition_{min}  \]
\[ \forall i \in \{1,...,n_{support}\}, \forall j \in \{1,...,n_{joints}\} : | evaluate(dK, J, \alpha, t_i)[j] | \le jointvelocity_{max}  \]
</p>
 

<h4 class="mt-5 mb-3"><span id="Lagrangian Loss Formulation"></span>Lagrangian Loss Formulation</h4>

<p>
During optimization, the constraint loss is transformed into a Lagrangian formulation and optimized using the Squared Penalty Method. This method iteratively minimizes the current loss and increases the Lagrangian multiplier by an order of magnitude until all constraints are satisfied.
</p>

<p>
The Constraint Loss Formulation is defined as

\[
\min_{\alpha} L(\alpha) = ObstacleLoss(\alpha) + \lambda_{sg} StartGoalLoss(\alpha) 
 + \lambda_{jl} JointLimitLoss(\alpha) + \lambda_{reg} || evaluate(K, J, \alpha, \cdot) ||^2_2   
\]
</p>
<br>
<br>
<p>
where the StartGoalLoss is defined as 
\[
StartGoalLoss(\alpha) = StartGoalPositionLoss(\alpha) + StartGoalVelocityLoss(\alpha)
\]
\[
StartGoalPositionLoss(\alpha) = {1 \over 2 } || evaluate(K, J, \alpha, t_1)  - config_{start} ||^2_2 + {1 \over 2 } || evaluate(K, J, \alpha, t_{n_{support}}) - config_{goal} ||^2_2
\]
\[
StartGoalVelocityLoss(\alpha) = {1 \over 2 } || evaluate(dK, J, \alpha, t_1) ||^2_2 + {1 \over 2 } || evaluate(dK, J, \alpha, t_{n_{support}}) ||^2_2
\]
</p>
<br>
<p>
Unlike to the StartGoalLoss, the JointLimitLoss is only applied if the limit is exceeded, thus, is a bit more complicated.
We chose to normalize the loss of the JointPositionLimit and JointVelocityLimit by using \(mean\_joint\_position = 0.5 (max\_joint\_position + min\_joint\_position)\), \(std\_joint\_position = 0.5 |max\_joint\_position - min\_joint\_position|\) and \(max\_joint\_velocity\) so that the loss of a constraint is 1 when the maximum or minimum is met and does not vary for different constraint values.
\[
JointLimitLoss(\alpha) = JointPositionLimitLoss(\alpha) + JointVelocityLimitLoss(\alpha)
\]
\[
JointPositionLimitLoss(\alpha) = \sum_{i=1}^{n_{support}} \sum_{j=1}^{n_{joints}} \left[ JointPositionLimitViolated(\alpha, i, j) \right] \left({ evaluate(K, J, \alpha, t_i)[j] - mean\_joint\_position \over std\_joint\_position } \right)^2
\]
\[
JointPositionLimitViolated(\alpha, i, j) = evaluate(K, J, \alpha, t_i)[j] \gt max\_joint\_position \vee evaluate(K, J, \alpha, t_i)[j] \lt min\_joint\_position 
\]
</p>
<br>
<p>
\[
JointVelocityLimitLoss(\alpha) = \sum_{i=1}^{n_{support}} \sum_{j=1}^{n_{joints}}  \left[ JointVelocityLimitViolated(\alpha, i, j) \right] \left({ evaluate(dK, J, \alpha, t_i)[j] | \over max\_joint\_velocity } \right)^2
\]
\[
JointVelocityLimitViolated(\alpha, i, j) = |evaluate(dK, J, \alpha, t_i)[j] | \gt max\_joint\_velocity
\]
</p>
<br>
<p>
For the optimization process, the obstacle loss is adjusted from the initial objective to overcome local optima and improve convergence.
The used obstacle loss combines the average and the maximum trajectory cost of the supporting points \(t_i\) with a hyperparameter \( \lambda_{max\_cost} \) and is defined as follows:

\[ ObstacleLoss(\alpha) = 
         \lambda_{max\_cost} \max_{i} ObstacleCost_{t_i}(\alpha)
  + (1 - \lambda_{max\_cost}) {1 \over n_{support}} \sum_{i=1}^{n_{support}} ObstacleCost_{t_i}(\alpha)
\]
\[
ObstacleCost_{t_i}(\alpha) = costmap( fk(evaluate(K, J, \alpha, t_i) )
\]

We analyze the effect of different \( \lambda_{max\_cost} \) values in the <a href="#Insights">Insights</a>.
</p>


<h2 class="mt-5 mb-3"><span id="Implementation"></span>Implementation</h2>

<p>
The Trajectory Optimization is implemented in Python using <a href="https://jax.readthedocs.io/en/latest/">JAX</a> and its <a href="https://jax.readthedocs.io/en/latest/jax.numpy.html">NumPy interface</a>.
JAX has two features we want to utilize in this work:
The <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html">automatic differentiation</a> of functions can speed up the development process as an adequate loss function can be designed without having to implement the analytical gradient each time to test it.
JAX can achieve very fast runtimes using its <a href="https://jax.readthedocs.io/en/latest/jit-compilation.html">just-in-time compilation</a> of Python code and thus being able to do trajectory optimization at 50 Hz.
</p>


<h4 class="mt-5 mb-3"><span id="Trajectory Initialization"></span>Trajectory Initialization</h4>

<p> 
We start the trajectory initialization by choosing \( n_{support} \) supporting timesteps \( t_i \) between 0 and 1, which are usually equally distanced.
The Kernel matrix \(K\) and \(dK\) are initialized with the RBF kernel 
\[
K[i, j] = \exp\left( - \frac{(t_i - t_j)^2}{2 \cdot \text{rbf_var}^2} \right)
\]
and it's derivative
\[
dK[i, j] = \frac{t_i - t_j}{\text{rbf_var}^2} \exp\left( - \frac{(t_i - t_j)^2}{2 \cdot \text{rbf_var}^2} \right)
\]
with the variance rbf_var.
</p>
<p>
The matrix \(J\) is initialized as the addition of the identity matrix and random Gaussian noise. The magnitude of the Gaussian noise influences the cross-joint communication and convergence speed. We achieved good robust results with a mean of 0.15.
</p>
<p>
The \(\alpha\) values are initialized by fitting the trajectory to 
\[
d(t) = config_{start} + c(t) * (config_{goal} - config_{start})  
\]
with a function 
\[
c(t) = 6  t^5 - 15 t^4 + 10 t^3
\]
where the function c was selected such that c(0)=0, c(1)=1, c'(0)=0, c'(1)=0, c''(0)=0, c''(1)=0 holds.
This initializes the trajectory to a straight line starting at \(config_{start}\) and finishing at \(config_{goal}\) while having zero velocity and acceleration at start and end.
</p>


<h4 class="mt-5 mb-3">Analytical Gradient</h4>

<p>
To speed up the trajectory optimization we do not use the automatic differentiation offered by JAX but implement the analytical gradient.
</p>
<p>
While the gradient computation of the various elements of our loss function is very straightforward, the complexity of the Lagrangian loss complicated the combination of sub-gradients to the final gradient.
During the implementation of the analytical gradient, we used two main supporting tools:
<br>
The newest model of ChatGPT GPT-4 was able to implement the analytical gradient of a provided function with hardly any mistakes. 
<br>
By comparing the result of our implementation to the one of the JAX automatic differentiation we could iteratively ensure the correctness of our gradient computation for each sub-function.
<br>
The combination of both tools made this part of this work a lot more pleasant and sped up the development significantly.
</p>

<h5 class="mt-5 mb-3">Optimizer</h5>

<p>
As described in the <a href="#Lagrangian Loss Formulation">Lagrangian Loss Formulation Section</a>, we use a dual optimization to find a minimal solution while holding the constraints.
For the dual optimization, especially with the lambda having different orders of magnitude, a fixed learning rate is not suitable to robustly find a solution.
Therefore we implemented two different optimizers for comparison.
<br>
The first optimizer uses regular Gradient Descent with a different learning rate for each outer iteration of the dual optimization to compensate for the increase of the lambdas.
The second optimizer uses Backtracking Line Search Gradient Descent and thus uses an adaptive learning rate that does not need any other fine-tuning.
<br>
In practice, the Backtracking Line Search Gradient Descent is more robust to different environments than the regular Gradient Descent due to the adaptive learning rate.
We will compare both optimizers in the experiments regarding runtime.
</p>


<!--

<h4 class="mt-5 mb-3">Algorithmic Pipeline</h4>
<h5 class="mt-3 mb-3">Pseudocode with linenumbers</h5>

	TODO 
<pre>
<code>
alpha, K, dK, J = initialize_trajectory()

while not constraint_fulfilled(alpha, K, dK, J) or n_outer==0 do

	while last_loss_reduction > eps do

		compute constraint_loss
		compute gradient
		lr = find optimal backtracking line search learning rate
		update trajectory: alpha = 

	
</code>
</pre>
-->



<h2 class="mt-5 mb-3"><span id="Experiments"></span>Experiments</h2>

<h4 class="mt-3 mb-3">Environment</h4>

<p>
Our Environment is defined by the \(config_{start}\) and \(config_{goal}\) of the robot, and several obstacles that need to be avoided and passed with maximal distance. 
</p><p>
For the obstacles, we define a cost map with the 
\[
costfunction(x) = \sum_{i=1}^{n_{obstacles}} {0.8 \over 0.5 + 0.5 ||x  - obstacle_i||^2_2 } 
\]
to get a convex optimization problem concerning a 2D point in the environment.
<br>
The environment, we use in our experiments, is shown below along with the gradient field of the cost map that maximizes the distance to the obstacles
</p>

<figure class="left">
	<img class="img-fluid" src="assets/images/obstacles_costmap.png" alt="image" width="50%">
	<img class="img-fluid" src="assets/images/gradientfield.png" alt="image" width="45%">
	<figcaption class="mt-2 text-center image-caption">Obstacles of our environment with the resulting cost map and the robots start and goal configuration on the left and Gradient Field of the cost map on the right</figcaption>
</figure>


<h4 class="mt-5 mb-3">Runtime</h4>

<p>
One goal of this project was to achieve a runtime of trajectory optimization enabling robot control at 50 Hz.
</p><p>
In our experiments, we initialize the optimizers and perform the just-in-time compilation, which takes around a second, before our runtime measurements.
As optimizers can solve dynamically changing environments (\(config_{start}\), \(config_{goal}\), and obstacles) without re-compilation, this approach is also possible for real robot deployment.
And therefore, we find excluding the compilation from the runtime measurement the most realistic approach.
</p>

<h5 class="mt-3 mb-3">Optimizers</h5>
<p>
As previously discussed we implemented two optimizers to solve the problem. Both optimizers use the same just-in-time compiled loss and gradient function. For both optimizers, we implemented two versions of the optimization loop. The first version uses regular Python code and the second one is adjusted to support just-in-time compilation by replacing <i>if</i> and <i>while</i> statements with constructs like <i><a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.cond.html">jax.lax.cond</a></i> and <i><a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.while_loop.html">jax.lax.while_loop</a></i>. 
We use the analytical gradient for this experiment and compare their runtimes in the next table. 
</p><p>
We see that the Backtracking Line Search approach is significantly faster than the Regular Gradient Descent, which we attribute to the fact that it needs almost only half the optimization steps (145 vs 259) due to its adaptive learning rate. 
Also, the JIT loop is a lot faster than the regular loop which was to be expected.
</p>

<table class="table table-bordered mb-5 table-center">
	<thead>
		<tr>
			<th scope="col">Runtimes with Mean <br> and Standard Deviation </th>
			<th scope="col">Backtracking Line Search <br> Gradient Descent</th>
			<th scope="col">Regular <br> Gradient Descent </th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>Regular Loop</th>
			<td> 34.81ms ( 1.03ms ) </td>
			<td> 122.59ms ( 1.31ms ) </td>
		</tr>
		<tr>
			<th>JIT Loop</th>
			<td> 3.12ms ( 0.01ms ) </td>
			<td> 7.26ms ( 0.06ms ) </td>
		</tr>
		
	</tbody>
</table>

<h5 class="mt-3 mb-3">Analytical Gradient and Automatic Differentiation</h5>
<p>
	We also want to evaluate the speedup of our analytical gradient compared to the <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html">automatic differentiation of JAX</a>.
	Here we observe only a small difference when using the JIT loop but a significant difference in the regular loop. 
	From analyzing the <a href="https://jax.readthedocs.io/en/latest/profiling.html">JAX profile</a> we can see that the automatic differentiation performs an explicit backward step in each iteration of the regular loop to discover the gradient path.
        In the JIT loop the explicit backward step is only done during compilation and then just recomputes the resulting gradient in the following iterations is therefore much faster. 
	We think that this is very interesting because the loss function has been just-in-time compiled so the gradient path should not be able to vary.
</p>

<table class="table table-bordered mb-5 table-center">
	<thead>
		<tr>
			<th scope="col">Runtimes with Mean <br> and Standart Deviation </th>
			<th scope="col"> Analytical <br> Gradient  </th>
			<th scope="col"> Automatic <br> Differentiation </th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th>Regular Loop</th>
			<td> 34.81ms ( 1.03ms ) </td>
			<td> 292.66ms ( 2.40ms ) </td>
		</tr>
		<tr>
			<th>JIT Loop</th>
			<td> 3.12ms ( 0.01ms ) </td>
			<td> 3.56ms ( 0.03ms ) </td>
		</tr>
		
	</tbody>
</table>

<h5 class="mt-3 mb-3">Loss and Gradient Function and its subparts</h5>
<p>
	Next, we want to analyze the runtime of the loss and gradient functions and how this is divided into the different subparts. 
	We again use the JAX profiler to obtain this information. Unfortunately, it is not possible to measure the runtime of parts of a JIT-compiled function, and therefore we have to use an uncompiled version of the loss and gradient functions to get the runtime of the JIT-compiled parts. These results are not the same as when the whole loss function is compiled, as some optimizations between multiple subfunctions are not possible. However, they provide an intuition of the relative runtimes.
</p><p>
	We show the results in the following plot. We see that the runtimes of the matching loss and gradient functions are often very similar, with the only significant difference being the Obstacle Loss. This is because the gradient of a maximum operator takes longer to compute than the maximum itself.
</p>

<div>
<figure class="blog-banner">
	<img class="img-fluid" src="assets/images/runtime_plot.png" alt="image" width="1000px">
</figure>
</div>

<p>
	While these functions only take a few microseconds in this experiment, the computation of the overall loss and gradient functions requires only 18.2 µs and 24.2 µs, respectively, when JIT-compiled. So, there is still a substantial speedup in the joint compilation, but we are unable to fully analyze it.
</p>


<h5 class="mt-3 mb-3">Scalability for different problem sizes</h5>

We also want to evaluate how well our solution scales for different problem sizes. In our case, the number of supporting points \(n_{support}\) influences the problem size. 
Since this number affects the size of the kernel matrices quadratically, it significantly impacts the runtimes.
While we can observe a linear runtime complexity for up to 100 supporting points, the runtime grows quadratically in larger problems.

<div width="800px>
	<figure class="blog-banner">
		<img class="img-fluid" src="assets/images/runtime_problemsize.png" alt="image" width="800px">
	</figure>
</div>


<h4 class="mt-5 mb-3">Videos</h4>

<p>
	In this section, we will show two videos of our trajectory optimization.
	<br>
	The first video shows the trajectory optimization process, starting at the straight-line initialization and converging to the optimal solution avoiding all the obstacles.
</p>

<div>
<img id="gif-1" src="assets/images/trajectory_series.gif" width="800px"
    onmouseover="document.getElementById('gif-1').src='assets/images/trajectory_series.gif'" 
/></div>


<p>
	and in the following video, you see the final robot movement of the optimized trajectory.
</p>

<div>
<img id="gif1" src="assets/images/trajectory_robotmovement.gif" width="800px"
    onmouseover="document.getElementById('gif1').src='assets/images/trajectory_robotmovement.gif'" 
/></div>

<!-- TODO Videos: final result yupiter -->






<h2 class="mt-5 mb-3"><span id="Insights"></span>Insights</h2>

<h4 class="mt-3 mb-3">Complete Robot Obstacle Avoidance</h4>

<p>
	Depending on the application, not only the end-effector but the whole robot may be placed in an environment with obstacles that need to be avoided. 
	In this case, the cost function can be easily adjusted to minimize the cost at the position of all joints:
\[ 
ObstacleCost_{t_i}(\alpha) = \sum_{j=1}^{n_{joints}} costmap( fk_j(evaluate(K, J, \alpha, t_i) ) 
\]
with \(fk_j\) being the forward kinematic for the joint \(j\) .
</p>


<h4 class="mt-5 mb-3">Lambda Max Cost</h4>

<p>
In our loss formulation, we introduced the trajectory obstacle cost as the weighted sum of the average obstacle cost and the maximum obstacle cost using \( \lambda_{max\_cost} \).
\[ 
	ObstacleLoss(\alpha) = 
	\lambda_{max\_cost} \max_{i} ObstacleCost_{t_i}(\alpha)
	+ (1 - \lambda_{max\_cost}) {1 \over n_{support}} \sum_{i=1}^{n_{support}} ObstacleCost_{t_i}(\alpha)
\]
The reason is that the optimization of the trajectory is no longer convex, as some cross-joint and cross-timestep adaptation is required for the trajectory to converge to a global optimum rather than getting stuck in a local optimum.
</p>
<p>
	We assume the gradient of the part of the trajectory close to the obstacles is not large enough to overcome the local optima of the cost constraints and the smoothness cost. Thus, the optimizer may find other ways to decrease the average trajectory cost without sufficiently increasing the distance to the nearest obstacles. 
	
	In the following figure, you can see the results of different \( \lambda_{max\_cost} \) values.
</p>

<figure class="blog-banner">
	<img class="img-fluid" src="assets/images/lambda_max_cost.png" alt="image" width="1000px">
	<figcaption class="mt-2 text-center image-caption">Different Trajectory Results for different \( \lambda_{max\_cost} \)</figcaption>
</figure>

<p>
We can see that the trajectory of the optimization without the max cost ( \( \lambda_{max\_cost} = 0 \) ) stays very close to the straight line initialization and does not find a suitable solution to increase the minimum distance to the obstacles. 
<br>
Although the other trajectories have a different objective, some still achieve lower average costs than \( \lambda_{max\_cost} = 0 \) and we therefore assume a local optimum present.
In the following table, you can see the average and maximum loss for the above-shown trajectories.
</p>

<table class="table table-striped mb-5 table-center ">
	<thead>
		<tr>
			<th scope="col">\( \lambda_{max\_cost} \)</th>
			<th scope="col">Average Cost</th>
			<th scope="col">Maximum Cost</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>0.0</td>
			<td>1.75</td>
			<td>3.37</td>
		</tr>
		<tr>
			<td>0.25</td>
			<td>1.64</td>
			<td>2.19</td>
		</tr>
		<tr>
			<td>0.5</td>
			<td>1.69</td>
			<td>2.19</td>
		</tr>
		<tr>
			<td>0.75</td>
			<td>1.73</td>
			<td>2.19</td>
		</tr>
		<tr>
			<td>1.0</td>
			<td>1.82</td>
			<td>2.19</td>
		</tr>
	</tbody>
</table>

<h4 class="mt-5 mb-3">Jacobian Matrix</h4>
<p>
	In our trajectory formulation, we use a matrix \(J\) to enable joint optimization with communication across different joints influencing each other. 
	While the original paper suggested using the robotic Jacobian matrix for this, we observed in our experiments that the optimization did not find a suitable solution when we used the actual Jacobian matrix. 
	We believe the reason is that the magnitude of the entries in a robotic Jacobian varies significantly. Naturally, the elements for the first joint are larger than those for the last joints, making any optimization with the same learning rate either impossible or at least very difficult. 
	However, since we observed that random cross-joint communication accelerates the trajectory optimization, we use a different matrix \(J\), as described in the <a href="#Trajectory Initialization">Trajectory Initialization Section</a>.
</p>

<h4 class="mt-5 mb-3">JAX Profile</h4>

<p>
Another important tool in our development was the <a href="https://jax.readthedocs.io/en/latest/profiling.html">JAX profile</a> we used during the runtime experiments.
While it is not very intuitive to understand at first, it can significantly aid in development. 
For example, we used it to detect when JAX recompiles functions as inputs change and to measure how long recompilation takes. 
Avoiding recompilation is crucial for achieving low runtimes, as recompilation often takes 100-1000ms. Additionally, it shows the number of calls and runtimes of functions, which we used in our experiments.
</p>

<h2 class="mt-5 mb-3"><span id="Conclusion"></span>Conclusion</h2>

<p>
	In this project, we developed a real-time capable trajectory optimization using functional gradients and JAX. 
	We discussed the advantages of the analytical gradient compared to the automatic differentiation provided by JAX and the impact of just-in-time compilation on the overall optimization process. 
	While a trajectory formulated in the RKHS is easier to optimize, the approach remains quite sensitive to hyperparameters. Nevertheless, we are confident that it can be used for real robot deployment.
</p>


			
<nav class="blog-nav nav nav-justified my-5">
	<a class="nav-link-top nav-item nav-link rounded" href="#">Back to Start<i class="arrow-prev fas fa-long-arrow-alt-up"></i><i class="arrow-next fas fa-long-arrow-alt-up"></i></a>
</nav>
	
</div><!--//container-->
</article>
	
	
<footer class="footer text-center py-2 theme-bg-dark">
	
	<!--/* This template is free as long as you keep the footer attribution link. If you'd like to use the template without the attribution link, you can buy the commercial license via our website: themes.3rdwavemedia.com Thank you for your support. :) */-->
	<small class="copyright">Blog based on template by <a href="https://themes.3rdwavemedia.com" target="_blank">Xiaoying Riley</a></small>
	
</footer>

</div><!--//main-wrapper-->
    

        
       
<!-- Javascript -->          
<script src="assets/plugins/popper.min.js"></script> 
<script src="assets/plugins/bootstrap/js/bootstrap.min.js"></script> 

<!-- Page Specific JS -->
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.5.1/highlight.min.js"></script>

<!-- Custom JS -->
<script src="assets/js/blog.js"></script> 


</body>
</html> 




<!--
<h2 class="mt-5 mb-3">Code Block Example</h2>
<p>You can get more info at <a class="text-link" href="https://highlightjs.org/" target="_blank">https://highlightjs.org/</a>. Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. Donec pede justo, fringilla vel, aliquet nec, vulputate eget, arcu. In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus. Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo ligula, porttitor eu, consequat vitae, eleifend ac, enim. </p>
<pre>
	<code>
function $initHighlight(block, cls) {
  try {
    if (cls.search(/\bno\-highlight\b/) != -1)
      return process(block, true, 0x0F) +
             ` class="${cls}"`;
  } catch (e) {
    /* handle exception */
  }
  for (var i = 0 / 2; i < classes.length; i++) {
    if (checkCondition(classes[i]) === undefined)
      console.log('undefined');
  }
}

export  $initHighlight;
	</code>
</pre>


<h2 class="mt-5 mb-3">Typography</h2>
<p>Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus.</p>
<h5 class="my-3">Bullet Points:</h5>
<ul class="mb-5">
	<li class="mb-2">Lorem ipsum dolor sit amet consectetuer.</li>
	<li class="mb-2">Aenean commodo ligula eget dolor.</li>
	<li class="mb-2">Aenean massa cum sociis natoque penatibus.</li>
</ul>
<ol class="mb-5">
	<li class="mb-2">Lorem ipsum dolor sit amet consectetuer.</li>
	<li class="mb-2">Aenean commodo ligula eget dolor.</li>
	<li class="mb-2">Aenean massa cum sociis natoque penatibus.</li>
</ol>
<h5 class="my-3">Quote Example:</h5>
<blockquote class="blockquote m-lg-5 py-3   ps-4 px-lg-5">
	<p class="mb-2">You might not think that programmers are artists, but programming is an extremely creative profession. It's logic-based creativity.</p>
	<footer class="blockquote-footer mt-0">John Romero</footer>
</blockquote>

<h5 class="my-3">Table Example:</h5>
<table class="table table-striped my-5">
	<thead>
		<tr>
			<th scope="col">#</th>
			<th scope="col">First</th>
			<th scope="col">Last</th>
			<th scope="col">Handle</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<th scope="row">1</th>
			<td>Mark</td>
			<td>Otto</td>
			<td>@mdo</td>
		</tr>
		<tr>
			<th scope="row">2</th>
			<td>Jacob</td>
			<td>Thornton</td>
			<td>@fat</td>
		</tr>
		<tr>
			<th scope="row">3</th>
			<td>Larry</td>
			<td>the Bird</td>
			<td>@twitter</td>
		</tr>
	</tbody>
</table>

<h5 class="mb-3">Embed A Tweet:</h5>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">1969:<br>-what&#39;re you doing with that 2KB of RAM?<br>-sending people to the moon<br><br>2017:<br>-what&#39;re you doing with that 1.5GB of RAM?<br>-running Slack</p>&mdash; I Am Devloper (@iamdevloper) <a href="https://twitter.com/iamdevloper/status/926458505355235328?ref_src=twsrc%5Etfw">November 3, 2017</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>



<h2 class="mt-5 mb-3">Video Example</h2>
<p>Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. </p>

<div class="ratio ratio-16x9">
	<iframe width="560" height="315" src="https://www.youtube.com/embed/1nxSE0R27Gg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>					
</div>

</div>
 -->